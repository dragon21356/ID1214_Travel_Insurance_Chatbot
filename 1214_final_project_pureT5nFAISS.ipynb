{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LVU1PBTPqJw7",
        "outputId": "b5e1ed2c-47d2-4119-bd4c-09a23e37906e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, markupsafe, ffmpy, faiss-cpu, colorlog, aiofiles, starlette, Mako, safehttpx, gradio-client, fastapi, alembic, optuna, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed Mako-1.3.8 aiofiles-23.2.1 alembic-1.14.0 colorlog-6.9.0 faiss-cpu-1.9.0.post1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 optuna-4.1.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.20 ruff-0.8.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio pandas numpy faiss-cpu torch sentence-transformers transformers optuna scikit-learn python-dotenv openpyxl typing-extensions tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AydgdKayysWg"
      },
      "source": [
        "            ### --- Intent Matching using FAISS or Keywords --- ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "BmgWFQJTq5DC",
        "outputId": "a8a3f8c9-d93d-4f43-ff12-79d45c5f897e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b15489dc569c62b099.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b15489dc569c62b099.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b15489dc569c62b099.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import logging\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_scheduler\n",
        "from typing import List, Optional, Dict, Any, Union\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InsuranceBot:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.sentence_model = None\n",
        "        self.t5_model = None\n",
        "        self.t5_tokenizer = None\n",
        "        self.embeddings = None\n",
        "        self.faiss_index = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.initialise_models()\n",
        "\n",
        "    def load_data(self, file_path: str) -> str:\n",
        "        try:\n",
        "            self.data = pd.read_excel(file_path)\n",
        "\n",
        "            # validating required columns\n",
        "            required_columns = [\n",
        "                'Insurance Provider', 'Plan Name', 'Premium Price (S$)',\n",
        "                'Overseas Medical Expenses (S$)', 'Trip Cancellation (S$)'\n",
        "            ]\n",
        "            missing_columns = [col for col in required_columns if col not in self.data.columns]\n",
        "            if missing_columns:\n",
        "                return f\"Missing required columns: {', '.join(missing_columns)}\"\n",
        "\n",
        "            # cleaning numeric values\n",
        "            for col in ['Premium Price (S$)', 'Overseas Medical Expenses (S$)', 'Trip Cancellation (S$)']:\n",
        "                self.data[col] = self.data[col].apply(self.clean_numeric_value)\n",
        "\n",
        "            # generating embeddings for FAISS\n",
        "            texts = [f\"{row['Insurance Provider']} {row['Plan Name']}\" for _, row in self.data.iterrows()]\n",
        "            self.embeddings = self.sentence_model.encode(texts, convert_to_tensor=True).cpu().numpy()\n",
        "            self.embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
        "\n",
        "            # FAISS indexing - inverted file index w clustering\n",
        "            nlist = min(5, len(self.embeddings) // 3)  #cluster==1/3rd of data points\n",
        "            self.faiss_index = faiss.IndexIVFFlat(\n",
        "                faiss.IndexFlatL2(self.embeddings.shape[1]),  #inner flat index\n",
        "                self.embeddings.shape[1],\n",
        "                nlist,  #dynamic clustering based on data size\n",
        "                faiss.METRIC_L2\n",
        "            )\n",
        "\n",
        "            # training only if enough points exist for clustering\n",
        "            if len(self.embeddings) >= nlist:\n",
        "                self.faiss_index.train(self.embeddings)\n",
        "                self.faiss_index.add(self.embeddings)\n",
        "            else:\n",
        "                logger.warning(\"Not enough data points to train FAISS clusters. Using Flat index.\")\n",
        "                self.faiss_index = faiss.IndexFlatL2(self.embeddings.shape[1])  #fallback to flat index\n",
        "                self.faiss_index.add(self.embeddings)\n",
        "\n",
        "            # fine-tuning search parameters\n",
        "            self.faiss_index.nprobe = 10  #search 10 clusters to balance between speed and recall\n",
        "\n",
        "            logger.info(f\"FAISS index size: {self.faiss_index.ntotal}\")\n",
        "            logger.info(f\"Sample Data: {self.data.head()}\")\n",
        "\n",
        "            return \"Data loaded successfully!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading data: {e}\")\n",
        "            return f\"Error loading data: {str(e)}\"\n",
        "\n",
        "    def fine_tune_t5(self, training_data: List[Dict[str, str]], epochs: int = 3, batch_size: int = 4, lr: float = 5e-5):\n",
        "        \"\"\"Fine-tune T5 model.\"\"\"\n",
        "        logger.info(\"Starting T5 fine-tuning...\")\n",
        "\n",
        "        dataset = InsuranceDataset(self.t5_tokenizer, training_data)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # optimizer and scheduler\n",
        "        optimizer = AdamW(self.t5_model.parameters(), lr=lr)\n",
        "        scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * epochs)\n",
        "\n",
        "        # Move model to device\n",
        "        self.t5_model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Move data to device\n",
        "                inputs = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.t5_model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            logger.info(f\"Epoch {epoch + 1}/{epochs} Loss: {loss.item()}\")\n",
        "\n",
        "        # Save fine-tuned model\n",
        "        self.t5_model.save_pretrained('fine_tuned_t5')\n",
        "        self.t5_tokenizer.save_pretrained('fine_tuned_t5')\n",
        "        logger.info(\"Fine-tuning completed and model saved.\")\n",
        "\n",
        "    def format_filtered_response(self, filtered_data: pd.DataFrame) -> str:\n",
        "        if filtered_data.empty:\n",
        "            return \"⚠️ No matching plans found based on your query.\"\n",
        "\n",
        "        response = \"📋 Matching Plans:\\n\\n\"\n",
        "        for _, row in filtered_data.iterrows():\n",
        "            response += (\n",
        "                f\"📋 {row['Insurance Provider']} - {row['Plan Name']}\\n\"\n",
        "                f\"💰 Premium: {self.format_currency(row['Premium Price (S$)'])}\\n\"\n",
        "                f\"🛡️ Coverage: {self.format_currency(row['Overseas Medical Expenses (S$)'])}\\n\"\n",
        "                f\"❌ Cancellation: {self.format_currency(row['Trip Cancellation (S$)'])}\\n\\n\"\n",
        "            )\n",
        "        return response\n",
        "\n",
        "    def format_faiss_response(self, results: List[Dict[str, Any]]) -> str:\n",
        "        if not results:\n",
        "            return \"⚠️ No relevant results found. Please refine your query.\"\n",
        "\n",
        "        response = \"📋 Similar Plans:\\n\\n\"\n",
        "        for r in results:\n",
        "            response += (\n",
        "                f\"📋 {r['Provider']} - {r['Plan']}\\n\"\n",
        "                f\"💰 Premium: {r['Premium']}\\n\"\n",
        "                f\"🛡️ Coverage: {r['Coverage']}\\n\"\n",
        "                f\"❌ Cancellation: {r['Cancellation']}\\n\\n\"\n",
        "            )\n",
        "        return response\n",
        "\n",
        "    # filtering data by a specific provider mentioned in the query\n",
        "    def filter_by_provider(self, provider: str) -> pd.DataFrame:\n",
        "        filtered = self.data[self.data['Insurance Provider'].str.contains(provider, case=False, na=False)]\n",
        "        return filtered\n",
        "\n",
        "    # processing the query and routing it to appropriate functions based on intent\n",
        "    def process_query(self, query: str) -> str:\n",
        "        try:\n",
        "            logger.info(f\"Processing query: {query}\")\n",
        "            if self.data is None:\n",
        "                return \"⚠️ Please upload the insurance data file before asking questions.\"\n",
        "\n",
        "            # Detect intent\n",
        "            intent = self.detect_intent(query)\n",
        "            logger.info(f\"Intent identified: {intent}\")\n",
        "\n",
        "            # Route query based on detected intent\n",
        "            if intent == \"affordable_plans\":\n",
        "                return self.most_affordable_plans()\n",
        "            elif intent == \"medical_coverage\":\n",
        "                return self.highest_coverage()\n",
        "            elif intent == \"compare_providers\":\n",
        "                return self.compare_providers(query)\n",
        "            elif intent == \"travel_recommendation\":\n",
        "                return self.recommend_travel_plan(query)\n",
        "\n",
        "            # Price range filters\n",
        "            price_match = re.findall(r'\\$?(\\d+)', query)\n",
        "            if len(price_match) == 2:  # Specific range\n",
        "                low, high = float(price_match[0]), float(price_match[1])\n",
        "                filtered = self.data[\n",
        "                    (self.data['Premium Price (S$)'] >= low) & (self.data['Premium Price (S$)'] <= high)\n",
        "                ]\n",
        "                return self.format_filtered_response(filtered)\n",
        "\n",
        "            elif len(price_match) == 1 and ('below' in query or 'under' in query):\n",
        "                limit = float(price_match[0])\n",
        "                filtered = self.data[self.data['Premium Price (S$)'] < limit]\n",
        "                return self.format_filtered_response(filtered)\n",
        "\n",
        "            elif len(price_match) == 1 and ('above' in query or 'over' in query):\n",
        "                limit = float(price_match[0])\n",
        "                filtered = self.data[self.data['Premium Price (S$)'] > limit]\n",
        "                return self.format_filtered_response(filtered)\n",
        "\n",
        "            # Fallback to FAISS similarity search\n",
        "            results = self.retrieve_similar(query, top_k=5)\n",
        "            return self.format_faiss_response(results)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {e}\")\n",
        "            return \"⚠️ Unable to process your query. Please try again.\"\n",
        "\n",
        "    # detects intent dynamically using FAISS or keyword-based matching\n",
        "    def detect_intent(self, query: str) -> str:\n",
        "        try:\n",
        "            logger.info(f\"Detecting intent for query: {query}\")\n",
        "\n",
        "            # Expanded predefined intents and examples\n",
        "            intent_examples = {\n",
        "                'cancellation_coverage': [\"compare trip cancellation coverage\", \"cancellation protection\"],\n",
        "                'baggage_protection': [\"baggage loss protection\", \"compare baggage coverage\"],\n",
        "                'affordable_plans': [\"affordable plans\", \"cheap travel insurance\", \"plans under $100\", \"cheapest plans\", \"budget plans\", \"lowest price\"],\n",
        "                'value_for_money': [\"best value for money plans\", \"cost-effective plans\"],\n",
        "                'compare_providers': [\"compare providers\", \"compare plans\", \"difference between providers\"],\n",
        "                'medical_coverage': [\"highest medical coverage\", \"compare medical coverage\"],\n",
        "                'travel_recommendation': [\"recommend a plan for travel\", \"best travel plan for vacation\"],\n",
        "                'top_plans': [\"top 3 plans\", \"best plans\", \"ranked plans\"],\n",
        "            }\n",
        "\n",
        "            # Prepare embeddings for examples\n",
        "            example_texts = []\n",
        "            intent_labels = []\n",
        "            for intent, examples in intent_examples.items():\n",
        "                example_texts.extend(examples)\n",
        "                intent_labels.extend([intent] * len(examples))\n",
        "\n",
        "            # Encode query and example embeddings\n",
        "            query_embedding = self.sentence_model.encode([query])[0]\n",
        "            example_embeddings = self.sentence_model.encode(example_texts)\n",
        "            similarities = np.dot(example_embeddings, query_embedding) / (\n",
        "                np.linalg.norm(example_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
        "            )\n",
        "\n",
        "            # Get best match and confidence\n",
        "            best_match_idx = np.argmax(similarities)\n",
        "            best_match_intent = intent_labels[best_match_idx]\n",
        "            confidence = similarities[best_match_idx]\n",
        "\n",
        "            logger.info(f\"Detected Intent: {best_match_intent}, Confidence: {confidence:.2f}\")\n",
        "\n",
        "            # Confidence threshold\n",
        "            if confidence > 0.6:\n",
        "                return best_match_intent\n",
        "            else:\n",
        "                return 'general'\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error detecting intent: {e}\")\n",
        "            return 'general'\n",
        "\n",
        "    def initialise_models(self):\n",
        "        logger.info(\"Initializing models...\")\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.sentence_model.to(self.device)\n",
        "\n",
        "        model_name = 't5-small'\n",
        "        self.t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.t5_model.to(self.device)\n",
        "        logger.info(\"Models initialized successfully.\")\n",
        "\n",
        "    def clean_numeric_value(self, value: Any) -> Union[float, str]:\n",
        "        try:\n",
        "            if pd.isna(value):\n",
        "                return 0.0\n",
        "            if isinstance(value, str) and 'unlimited' in value.lower():  #treat 'Unlimited' as high value\n",
        "                return 1e12\n",
        "            cleaned = re.sub(r'[^\\d.]', '', str(value))\n",
        "            return float(cleaned) if cleaned else 0.0\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error cleaning value {value}: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def format_currency(self, value: Union[float, str]) -> str:\n",
        "        if isinstance(value, str) and value.lower() == 'unlimited':\n",
        "            return 'Unlimited'\n",
        "        if isinstance(value, (int, float)):\n",
        "            return f\"S${value:,.2f}\"\n",
        "        return 'S$0.00'\n",
        "\n",
        "    # get insurance plans based on query, price filters and contextual notes\n",
        "    def retrieve_similar(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            logger.info(f\"Retrieving similar plans for query: {query}\")\n",
        "\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.sentence_model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
        "            query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "            query_embedding = query_embedding.reshape(1, -1)  # Ensure correct shape\n",
        "\n",
        "            # Normalize embeddings for FAISS\n",
        "            faiss.normalize_L2(self.embeddings)\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "\n",
        "            # Search FAISS index\n",
        "            distances, indices = self.faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "            # Collect matching results\n",
        "            results = []\n",
        "            for idx in indices[0]:\n",
        "                if idx < len(self.data):\n",
        "                    row = self.data.iloc[idx]\n",
        "                    results.append({\n",
        "                        'Provider': row['Insurance Provider'],\n",
        "                        'Plan': row['Plan Name'],\n",
        "                        'Premium': self.format_currency(row['Premium Price (S$)']),\n",
        "                        'Coverage': self.format_currency(row['Overseas Medical Expenses (S$)']),\n",
        "                        'Cancellation': self.format_currency(row['Trip Cancellation (S$)']),\n",
        "                        'Notes': row.get('Special Notes', \"No additional notes.\")\n",
        "                    })\n",
        "\n",
        "            # Sort results by price for affordability-related queries\n",
        "            if \"cheap\" in query.lower() or \"affordable\" in query.lower():\n",
        "                results = sorted(results, key=lambda x: float(re.sub(r'[^\\d.]', '', x['Premium'])))\n",
        "\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving similar plans: {e}\")\n",
        "            return []\n",
        "\n",
        "    ### --- Coverage Analysis --- ###\n",
        "\n",
        "    def cancellation_coverage(self) -> str:\n",
        "        \"\"\"Compares trip cancellation coverage across providers.\"\"\"\n",
        "        try:\n",
        "            sorted_plans = self.data.sort_values('Trip Cancellation (S$)', ascending=False).head(5)\n",
        "            response = \"✈️ Top Plans for Cancellation Coverage:\\n\\n\"\n",
        "            for _, plan in sorted_plans.iterrows():\n",
        "                response += (f\"📋 {plan['Plan Name']} ({plan['Insurance Provider']})\\n\"\n",
        "                            f\"💰 Premium: {self.format_currency(plan['Premium Price (S$)'])}\\n\"\n",
        "                            f\"❌ Cancellation: {self.format_currency(plan['Trip Cancellation (S$)'])}\\n\\n\")\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in cancellation coverage comparison: {e}\")\n",
        "            return \"⚠️ Unable to retrieve cancellation coverage details.\"\n",
        "\n",
        "    def baggage_protection(self) -> str:\n",
        "        \"\"\"Compares plans for baggage loss protection.\"\"\"\n",
        "        try:\n",
        "            if 'Baggage Loss (S$)' in self.data.columns:\n",
        "                sorted_plans = self.data.sort_values('Baggage Loss (S$)', ascending=False).head(5)\n",
        "                response = \"🛄 Top Plans for Baggage Loss Protection:\\n\\n\"\n",
        "                for _, plan in sorted_plans.iterrows():\n",
        "                    response += (f\"📋 {plan['Plan Name']} ({plan['Insurance Provider']})\\n\"\n",
        "                                f\"💰 Premium: {self.format_currency(plan['Premium Price (S$)'])}\\n\"\n",
        "                                f\"🛄 Baggage Loss: {self.format_currency(plan['Baggage Loss (S$)'])}\\n\\n\")\n",
        "                return response\n",
        "            return \"⚠️ Baggage loss coverage information is not available in the data.\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in baggage protection comparison: {e}\")\n",
        "            return \"⚠️ Unable to retrieve baggage protection details.\"\n",
        "\n",
        "\n",
        "    ### --- Price Analysis --- ###\n",
        "\n",
        "    def most_affordable_plans(self) -> str:\n",
        "        \"\"\"Lists the most affordable plans.\"\"\"\n",
        "        try:\n",
        "            plans = self.data.nsmallest(5, 'Premium Price (S$)')\n",
        "            response = \"💰 Most Affordable Plans:\\n\\n\"\n",
        "            for _, plan in plans.iterrows():\n",
        "                response += (f\"📋 {plan['Plan Name']} ({plan['Insurance Provider']})\\n\"\n",
        "                            f\"💰 Premium: {self.format_currency(plan['Premium Price (S$)'])}\\n\\n\")\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in affordable plans retrieval: {e}\")\n",
        "            return \"⚠️ Unable to retrieve affordable plans.\"\n",
        "\n",
        "    def value_for_money(self) -> str:\n",
        "        self.data['Value Ratio'] = self.data['Overseas Medical Expenses (S$)'] / self.data['Premium Price (S$)']\n",
        "        best_value = self.data.nlargest(3, 'Value Ratio')\n",
        "\n",
        "        response = \"💎 Best Value-for-Money Plans:\\n\\n\"\n",
        "        for _, plan in best_value.iterrows():\n",
        "            response += (\n",
        "                f\"📋 {plan['Plan Name']} - Value Ratio: {plan['Value Ratio']:.2f}\\n\"\n",
        "                f\"💰 Premium: {self.format_currency(plan['Premium Price (S$)'])}\\n\"\n",
        "                f\"🛡️ Coverage: {self.format_currency(plan['Overseas Medical Expenses (S$)'])}\\n\"\n",
        "            )\n",
        "        return response\n",
        "\n",
        "\n",
        "    ### --- Plan Comparisons --- ###\n",
        "\n",
        "    def compare_providers(self, query: str) -> str:\n",
        "        \"\"\"Compare two or more providers based on key metrics.\"\"\"\n",
        "        try:\n",
        "            providers = [p for p in self.data['Insurance Provider'].unique() if p.lower() in query.lower()]\n",
        "            if len(providers) < 2:\n",
        "                return \"⚠️ Please specify at least two providers to compare.\"\n",
        "\n",
        "            comparisons = []\n",
        "            for provider in providers:\n",
        "                provider_data = self.data[self.data['Insurance Provider'] == provider]\n",
        "                avg_premium = provider_data['Premium Price (S$)'].mean()\n",
        "                max_coverage = provider_data['Overseas Medical Expenses (S$)'].max()\n",
        "                max_cancellation = provider_data['Trip Cancellation (S$)'].max()\n",
        "\n",
        "                comparisons.append(\n",
        "                    f\"📊 {provider}\\n💰 Avg Premium: {self.format_currency(avg_premium)}\\n\"\n",
        "                    f\"🛡️ Max Coverage: {self.format_currency(max_coverage)}\\n\"\n",
        "                    f\"❌ Max Cancellation: {self.format_currency(max_cancellation)}\\n\\n\"\n",
        "                )\n",
        "            return \"\\n\".join(comparisons)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error comparing providers: {e}\")\n",
        "            return \"⚠️ Unable to process your comparison query. Please try again.\"\n",
        "\n",
        "\n",
        "    ### --- Travel Recommendations --- ###\n",
        "\n",
        "    def recommend_travel_plan(self, query: str) -> str:\n",
        "        try:\n",
        "            duration_match = re.search(r'(\\d+)\\s*(weeks?|days?)', query.lower())\n",
        "            duration = int(duration_match.group(1)) if duration_match else 1\n",
        "\n",
        "            if duration >= 3:\n",
        "                plans = self.data.nlargest(3, 'Overseas Medical Expenses (S$)')\n",
        "            else:\n",
        "                plans = self.data.nsmallest(3, 'Premium Price (S$)')\n",
        "\n",
        "\n",
        "            response = \"🌍 Recommended Plans for Travel:\\n\"\n",
        "            for _, plan in plans.iterrows():\n",
        "                response += (f\"📋 {plan['Plan Name']} - Premium: {self.format_currency(plan['Premium Price (S$)'])}, \"\n",
        "                            f\"Coverage: {self.format_currency(plan['Overseas Medical Expenses (S$)'])}\\n\")\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error recommending travel plans: {e}\")\n",
        "            return \"⚠️ Unable to recommend travel plans. Please try again.\"\n",
        "\n",
        "    def compare_providers(self, query: str) -> str:\n",
        "        try:\n",
        "            # search providers mentioned in the query\n",
        "            providers = [p for p in self.data['Insurance Provider'].unique() if p.lower() in query.lower()]\n",
        "            if len(providers) < 2:\n",
        "                return \"⚠️ Please specify at least two providers to compare.\"\n",
        "\n",
        "            # get comparison data\n",
        "            comparisons = []\n",
        "            for provider in providers:\n",
        "                provider_data = self.data[self.data['Insurance Provider'] == provider]\n",
        "\n",
        "                avg_premium = provider_data['Premium Price (S$)'].mean()\n",
        "                max_coverage = provider_data['Overseas Medical Expenses (S$)'].max()\n",
        "                max_cancellation = provider_data['Trip Cancellation (S$)'].max()\n",
        "\n",
        "                # add provider summary\n",
        "                comparisons.append(\n",
        "                    f\"📊 {provider}\\n💰 Avg Premium: {self.format_currency(avg_premium)}\\n\"\n",
        "                    f\"🛡️ Max Coverage: {self.format_currency(max_coverage)}\\n\"\n",
        "                    f\"❌ Max Cancellation: {self.format_currency(max_cancellation)}\"\n",
        "                )\n",
        "            return \"\\n\\n\".join(comparisons)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error comparing providers: {e}\", exc_info=True)\n",
        "            return \"⚠️ Unable to process your comparison query. Please try again.\"\n",
        "\n",
        "    # highest medical coverage\n",
        "    def highest_coverage(self) -> str:\n",
        "        plans = self.data.nlargest(3, 'Overseas Medical Expenses (S$)')\n",
        "        response = \"🏥 Plans with Highest Medical Coverage:\\n\\n\"\n",
        "        for _, plan in plans.iterrows():\n",
        "            response += f\"📋 {plan['Plan Name']} - Coverage: {self.format_currency(plan['Overseas Medical Expenses (S$)'])}\\n\"\n",
        "        return response\n",
        "\n",
        "class InsuranceDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        inputs = self.tokenizer(item['query'], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        outputs = self.tokenizer(item['response'], max_length=150, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'labels': outputs['input_ids'].squeeze(0)\n",
        "        }\n",
        "\n",
        "def handle_query(query: str, file: Optional[gr.File]) -> str:\n",
        "    # Check if a file is uploaded and load it\n",
        "    if file:\n",
        "        result = bot.load_data(file.name)\n",
        "        if \"successfully\" not in result.lower():\n",
        "            return f\"❌ Data loading failed: {result}\"\n",
        "    return bot.process_query(query)\n",
        "\n",
        "\n",
        "bot = InsuranceBot()\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=handle_query,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Type your question below.\"),\n",
        "        gr.File(label=\"Upload Excel File\", file_types=[\".xlsx\", \".xls\"])\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Response\"),\n",
        "    title=\"Travel Insurance Chatbot\",\n",
        "    description=\"Ask questions about insurance plans, providers, pricing, coverage and comparisons.\",\n",
        "    examples=[\n",
        "        [\"Tell me about FWD travel insurance plans\"],\n",
        "        [\"Compare trip cancellation coverage across providers\"],\n",
        "        [\"What are the best plans to take for a family trip\"],\n",
        "        [\"Compare AXA and AIA travel insurance plans\"],\n",
        "        [\"What are the cheapest travel insurance options?\"]\n",
        "    ],\n",
        "    theme=\"default\",\n",
        ")\n",
        "\n",
        "interface.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFxvLXdtwlVc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}